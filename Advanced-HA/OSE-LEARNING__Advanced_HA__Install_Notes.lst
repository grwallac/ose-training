**** ADVANCED HA -- Install Notes ***

(1) create repo list
      cat << EOF > /etc/yum.repos.d/open.repo
      [rhel-7-server-rpms]
      name=Red Hat Enterprise Linux 7
      baseurl=http://oselab.example.com/repos/3.3/rhel-7-server-rpms http://www.opentlc.com/repos/ose/3.3/rhel-7-server-rpms
      enabled=1
      gpgcheck=0

      # This repo is added for the OPENTLC environment not OSE
      [rhel-7-server-rh-common-rpms]
      name=Red Hat Enterprise Linux 7 Common
      baseurl=http://oselab.example.com/repos/3.3/rhel-7-server-rh-common-rpms http://www.opentlc.com/repos/ose//3.3/rhel-7-server-rh-common-rpms
      enabled=1
      gpgcheck=0

      [rhel-7-server-extras-rpms]
      name=Red Hat Enterprise Linux 7 Extras
      baseurl=http://oselab.example.com/repos/3.3/rhel-7-server-extras-rpms http://www.opentlc.com/repos/ose//3.3/rhel-7-server-extras-rpms
      enabled=1
      gpgcheck=0

      [rhel-7-server-optional-rpms]
      name=Red Hat Enterprise Linux 7 Optional
      baseurl=http://oselab.example.com/repos/3.3/rhel-7-server-optional-rpms http://www.opentlc.com/repos/ose//3.3/rhel-7-server-optional-rpms
      enabled=1
      gpgcheck=0

      [rhel-7-server-ose-3.3-rpms]
      name=Red Hat Enterprise Linux 7 OSE 3.3
      baseurl=http://oselab.example.com/repos/3.3/rhel-7-server-ose-3.3-rpms http://www.opentlc.com/repos/ose//3.3/rhel-7-server-ose-3.3-rpms
      enabled=1
      gpgcheck=0

      EOF

(2) prep repositories - locally
      yum clean all && yum repolist

(3) Create /etc/ansible/hosts.basic

    [cluster]
    master1.example.com
    master2.example.com
    master3.example.com
    infranode1.example.com
    infranode2.example.com
    infranode3.example.com
    node1.example.com
    node3.example.com
    node2.example.com
    node4.example.com
    node5.example.com
    node6.example.com

    [masters]
    master1.example.com
    master2.example.com
    master3.example.com

    [others]
    loadbalancer.example.com
    idm.example.com
    cf.example.com

(4) Check and distribute above as needed
      ansible -i /etc/ansible/hosts.basic cluster -m shell -a 'ls -lFa /etc/yum.repos.d/*.repo' -o

if copy needed:
      ansible master1.example.com -m copy -a "src=/etc/yum.repos.d/open.repo dest=/etc/yum.repos.d/open.repo"
      ansible master1.example.com -m shell -a "yum clean all && yum repolist"

(5) grab CA cert
You can get the ipa-ca.crt file from our provided ipa server (idm.example.com) using the following command:
      # wget http://idm.example.com/ipa/config/ca.crt -O /etc/origin/master/ipa-ca.crt
      wget http://idm.example.com/ipa/config/ca.crt -O /root/ipa-ca.crt

(6) Download Router certs:
       mkdir /root/certs ; http://www.opentlc.com/download/ose_fastadv/resources/3.2/router-certs.tar -O /root/certs/router-certs.tar ; tar xf /root/certs/router-certs.tar -C /root/certs

(7) Install TOOL CHAIN
      yum -y install atomic-openshift-utils

(8) configure DNS load balancer (loadbalancer.example.com)

          #!/bin/bash

          guid=`hostname|cut -f2 -d-|cut -f1 -d.`
          yum -y install bind bind-utils
          systemctl enable named
          systemctl stop named

          ### firewalld was being a bit problematic
          ### Since we turn it off later anyway, I've skipped this step.
          #firewall-cmd --permanent --zone=public --add-service=dns
          #firewall-cmd --reload
          #sleep 100;

          infraIP1=`host infranode1-$guid.oslab.opentlc.com ipa.opentlc.com  | grep $guid | awk '{ print $4 }'`
          infraIP2=`host infranode2-$guid.oslab.opentlc.com ipa.opentlc.com  | grep $guid | awk '{ print $4 }'`
          domain="cloudapps-$guid.oslab.opentlc.com"

            10  ; minimum (10 seconds)
          )
            NS master.${domain}.
          \$ORIGIN ${domain}.
          test A ${infraIP1}
          * A ${infraIP1}
          * A ${infraIP2}"  >  /var/named/zones/${domain}.db

          chgrp named -R /var/named
          chown named -R /var/named/zones
          restorecon -R /var/named

          echo "// named.conf
          options {
            listen-on port 53 { any; };
            directory \"/var/named\";
            dump-file \"/var/named/data/cache_dump.db\";
            statistics-file \"/var/named/data/named_stats.txt\";
            memstatistics-file \"/var/named/data/named_mem_stats.txt\";
            allow-query { any; };
            recursion yes;
            /* Path to ISC DLV key */
            bindkeys-file \"/etc/named.iscdlv.key\";
          };
          logging {
            channel default_debug {
              file \"data/named.run\";
              severity dynamic;
            };
          };
          zone \"${domain}\" IN {
            type master;
            file \"zones/${domain}.db\";
            allow-update { key ${domain} ; } ;
          };" > /etc/named.conf

          chown root:named /etc/named.conf
          restorecon /etc/named.conf

          systemctl start named

          dig @127.0.0.1 test.cloudapps-$guid.oslab.opentlc.com

          if [ $? = 0 ]
          then
            echo "DNS Setup was successful!"
          else
            echo "DNS Setup failed"
          fi

          echo Fully Finished the $0 script  | tee -a /root/.dns.installer.txt

          yum install iptables-services -y
          systemctl stop firewalld
          systemctl disable firewalld
          systemctl enable iptables
          systemctl start iptables

          iptables -I INPUT -p tcp --dport 53 -j ACCEPT
          iptables -I INPUT -p udp --dport 53 -j ACCEPT
          iptables-save > /etc/sysconfig/iptables

(9) ensure Docker is installed and running on all HOSTs.
      ansible -i /etc/ansible/hosts.basic  cluster -a "systemctl status docker"
    if needed:
      ansible -i /etc/ansible/hosts.basic  cluster -a "systemctl stop docker"
      ansible -i /etc/ansible/hosts.basic  cluster -a "systemctl start docker"

or
      ansible -i /etc/ansible/hosts.basic  cluster -a "service docker status"|grep -v com

and if needed:
      yum install docker-1.10.3 or// ansible nodes -a "yum -y install docker-1.10.3"

## on MASTER(s) -- /etc/sysconfig/docker:
      sed -i.bak -e 's/^OPTIONS/#OPTIONS/' -e '/^#OPTIONS/a OPTIONS=--insecure-registry 172.19.0.0/16' /etc/sysconfig/docker
      systemctl start docker
      systemctl enable docker

(10) create the Ansible hosts file for the INSTALLATION, FOLLOWING FROM INSTALLER SCRIPT AS IT PROBED FOR PROVIDED HOSTS, it's including IP Data which we've not specified elsewhere.  GOOD: in case DNS fails; BAD: can't move node's setup to another physical/virtual entity since IP and hostname are associated.  This section can be IGNORED as is GOOD FOR REFERENCE -- better configuration starts on LINE 231

      [OSEv3:children]
      nodes
      masters
      nfs
      etcd
      lb

      [OSEv3:vars]
      ansible_ssh_user=root
      openshift_master_cluster_method=native
      openshift_master_cluster_hostname=loadbalancer.example.com
      openshift_master_cluster_public_hostname=loadbalancer.example.com
      deployment_type=openshift-enterprise

      [nodes]
      master1  openshift_ip=192.168.0.101 openshift_public_ip=192.168.0.101 openshift_hostname=master1.example.com openshift_public_hostname=master1.example.com openshift_node_labels="{'region': 'infra'}" openshift_schedulable=False
      master2  openshift_ip=192.168.0.102 openshift_public_ip=192.168.0.102 openshift_hostname=master2.example.com openshift_public_hostname=master2.example.com openshift_node_labels="{'region': 'infra'}" openshift_schedulable=False
      master3  openshift_ip=192.168.0.103 openshift_public_ip=192.168.0.103 openshift_hostname=master3.example.com openshift_public_hostname=master3.example.com openshift_schedulable=False
      node1  openshift_ip=192.168.0.201 openshift_public_ip=192.168.0.201 openshift_hostname=node1.example.com openshift_public_hostname=node1.example.com openshift_schedulable=True
      node2  openshift_ip=192.168.0.202 openshift_public_ip=192.168.0.202 openshift_hostname=node2.example.com openshift_public_hostname=node2.example.com openshift_schedulable=True
      node3  openshift_ip=192.168.0.203 openshift_public_ip=192.168.0.203 openshift_hostname=node3.example.com openshift_public_hostname=node3.example.com openshift_schedulable=True
      node4  openshift_ip=192.168.0.204 openshift_public_ip=192.168.0.204 openshift_hostname=node4.example.com openshift_public_hostname=node4.example.com openshift_schedulable=True
      node5  openshift_ip=192.168.0.205 openshift_public_ip=192.168.0.205 openshift_hostname=node5.example.com openshift_public_hostname=node5.example.com openshift_schedulable=True
      node6  openshift_ip=192.168.0.206 openshift_public_ip=192.168.0.206 openshift_hostname=node6.example.com openshift_public_hostname=node6.example.com openshift_schedulable=True
      infranode1  openshift_ip=192.168.0.251 openshift_public_ip=192.168.0.251 openshift_hostname=infranode1-5c90.oslab.opentlc.com openshift_public_hostname=infranode1-5c90.oslab.opentlc.com openshift_schedulable=True
      infranode2  openshift_ip=192.168.0.252 openshift_public_ip=192.168.0.252 openshift_hostname=infranode2-5c90.oslab.opentlc.com openshift_public_hostname=infranode2-5c90.oslab.opentlc.com openshift_schedulable=True
      infranode3  openshift_ip=192.168.0.253 openshift_public_ip=192.168.0.253 openshift_hostname=infranode3-5c90.oslab.opentlc.com openshift_public_hostname=infranode3-5c90.oslab.opentlc.com openshift_schedulable=True


      [masters]
      master1  openshift_ip=192.168.0.101 openshift_public_ip=192.168.0.101 openshift_hostname=master1.example.com openshift_public_hostname=master1.example.com
      master2  openshift_ip=192.168.0.102 openshift_public_ip=192.168.0.102 openshift_hostname=master2.example.com openshift_public_hostname=master2.example.com
      master3  openshift_ip=192.168.0.103 openshift_public_ip=192.168.0.103 openshift_hostname=master3.example.com openshift_public_hostname=master3.example.com

      [nfs]
      master1  openshift_ip=192.168.0.101 openshift_public_ip=192.168.0.101 openshift_hostname=master1.example.com openshift_public_hostname=master1.example.com

      [etcd]
      master1  openshift_ip=192.168.0.101 openshift_public_ip=192.168.0.101 openshift_hostname=master1.example.com openshift_public_hostname=master1.example.com
      master2  openshift_ip=192.168.0.102 openshift_public_ip=192.168.0.102 openshift_hostname=master2.example.com openshift_public_hostname=master2.example.com
      master3  openshift_ip=192.168.0.103 openshift_public_ip=192.168.0.103 openshift_hostname=master3.example.com openshift_public_hostname=master3.example.com

      [lb]
      loadbalancer1.example.com


#### REPLACE $GUID in following lines !!!! ##########
### 264, 267, 271,

      cat << EOF > /etc/ansible/hosts
      # Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
      # The lb group lets Ansible configure HAProxy as the load balancing solution.
      # Comment lb out if your load balancer is preconfigured.
      [OSEv3:children]
      masters
      nodes
      etcd
      lb
      nfs

      # Set variables common for all OSEv3 hosts
      [OSEv3:vars]
      ansible_ssh_user=root
      deployment_type=openshift-enterprise

      ###
      ### EDIT AS NEEDED
      ###
      # LDAP auth
      openshift_master_identity_providers=[{'name': 'idm', 'challenge': 'true', 'login': 'true', 'kind': 'LDAPPasswordIdentityProvider', 'attributes': {'id': ['dn'], 'email': ['mail'], 'name': ['cn'], 'preferredUsername': ['uid']}, 'bindDN': 'uid=admin,cn=users,cn=accounts,dc=example,dc=com', 'bindPassword': 'r3dh4t1!', 'ca': '/etc/origin/master/ipa-ca.crt', 'insecure': 'false', 'url': 'ldap://idm.example.com/cn=users,cn=accounts,dc=example,dc=com?uid?sub?(memberOf=cn=ose-user,cn=groups,cn=accounts,dc=example,dc=com)'}]
      openshift_master_ldap_ca_file=/root/ipa-ca.crt

      # Native high availability cluster method with optional load balancer.
      # If no lb group is defined installer assumes that a load balancer has
      # been preconfigured. For installation the value of
      # openshift_master_cluster_hostname must resolve to the load balancer
      # or to one or all of the masters defined in the inventory if no load
      # balancer is present.
      openshift_master_cluster_method=native
      openshift_master_cluster_hostname=loadbalancer1.example.com
      openshift_master_cluster_public_hostname=loadbalancer1-$GUID.oslab.opentlc.com

      # default subdomain to use for exposed routes
      openshift_master_default_subdomain=cloudapps-$GUID.oslab.opentlc.com

      # Configure metricsPublicURL in the master config for cluster metrics
      # See: https://docs.openshift.com/enterprise/latest/install_config/cluster_metrics.html
      openshift_master_metrics_public_url=https://metrics.cloudapps-$GUID.oslab.opentlc.com

      # Enable cockpit
      osm_use_cockpit=false

      # default project node selector
      osm_default_node_selector='region=primary'
      openshift_hosted_router_selector='region=infra'
      openshift_hosted_router_replicas=1
      openshift_hosted_router_certificate={"certfile": "/root/certs/router.crt", "keyfile": "/root/certs/router.key", "cafile": "/root/certs/router-ca.crt"}
      openshift_hosted_registry_selector='region=infra'
      openshift_hosted_registry_replicas=1

      openshift_hosted_registry_storage_kind=nfs
      openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
      openshift_hosted_registry_storage_host=oselab.example.com
      openshift_hosted_registry_storage_nfs_directory=/srv/nfs
      openshift_hosted_registry_storage_volume_name=registry
      openshift_hosted_registry_storage_volume_size=5Gi

      # Force setting of system hostname when configuring OpenShift
      # This works around issues related to installations that do not have valid dns
      # entries for the interfaces attached to the host.
      openshift_set_hostname=True

      [nfs]
      oselab.example.com

      # host group for masters
      [masters]
      master1.example.com
      master2.example.com
      master3.example.com

      # host group for etcd
      [etcd]
      master1.example.com
      master2.example.com
      master3.example.com

      # Specify load balancer host
      [lb]
      loadbalancer1.example.com

      # host group for nodes, includes region info
      [nodes]
      master[1:3].example.com
      infranode1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'THX-1138'}" openshift_hostname=infranode1.example.com
      infranode2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'THX-1139'}" openshift_hostname=infranode2.example.com
      node[1:3].example.com openshift_node_labels="{'region': 'primary', 'zone': 'THX-1138'}"
      node[4:6].example.com openshift_node_labels="{'region': 'primary', 'zone': 'THX-1139'}"
      EOF

(11) BUILD it so they will come
      ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml

(12) once completed, connect to a MASTER node
      ssh root@master1.example.com
      oc get nodes --show-labels

(13) Configure DEFAULT namespace
      oc annotate namespace default openshift.io/node-selector='region=infra' --overwrite
Check:
      oc get namespace default -o yaml | grep infra

(14) Navigate your browse to
      https://loadbalancer1.opentlc.com:8443
        username: user1
        pass: openshift

(15) Configure HA NFS-Backed Registry and HAProxy Router
    Persistant backend storage is already deployed:
           grep '\[nfs\]' -A1  /etc/ansible/hosts
    check status:
          oc get pods
    and WAIT for propogation/image sync.

    NOTE: if you need to recreate the registry, manually:
          oadm registry --replicas=1 --create --credentials=/etc/origin/master/openshift-registry.kubeconfig
    run:
          oc status
    check the registry's service port:
          curl -v 172.30.41.32:5000/healthz  NOTE: connect data from above 'svc/docker-registry ' line

    oc get service docker-registry --template '{{.spec.portalIP}}:{{index .spec.ports 0 "port"}}/healthz'

(16) Create PV and PVC registry, from @master1

      echo '{
        "apiVersion": "v1",
        "kind": "PersistentVolume",
        "metadata": {
          "name": "registry-volume"
        },
        "spec": {
          "capacity": {
              "storage": "3Gi"
              },
          "accessModes": [ "ReadWriteMany" ],
          "nfs": {
              "path": "/srv/nfs/registry",
              "server": "oselab-'"$GUID"'.oslab.opentlc.com"
          }
        }
      }' | oc create -f -

(17) create PersistentVolunmeClaim, from @master1

      echo '{
        "apiVersion": "v1",
        "kind": "PersistentVolumeClaim",
        "metadata": {
          "name": "registry-claim"
        },
        "spec": {
          "accessModes": [ "ReadWriteMany" ],
          "resources": {
            "requests": {
              "storage": "3Gi"
            }
          }
        }
      }' | oc create -n default -f -

(18) Update registry volume and Scale, from @master1:
      oc volume dc/docker-registry --add --overwrite -t persistentVolumeClaim --claim-name=registry-claim --name=registry-storage

      oc scale dc/docker-registry --replicas=2oc scale dc/docker-registry --replicas=2

(19) because of NFS file attribute sybc issues, from @master1:
      oc get -o yaml svc docker-registry | \
            sed 's/(sessionAffinity:s**).*/1ClientIP/' | \
            oc replace -f -

(20) Determine where pods are deployed
      oc get pod -o wide

(21 - OPTIONAL) Redeploy ROUTER
    (21a)
          oc delete dc/router svc/router
    (21b)
          CA=/etc/origin/master
    (21c)
          oadm ca create-server-cert --signer-cert=$CA/ca.crt \
                 --signer-key=$CA/ca.key --signer-serial=$CA/ca.serial.txt \
                 --hostnames='*.cloudapps-$GUID.oslab.opentlc.com' \
                 --cert=cloudapps.crt --key=cloudapps.key
    (21d)  key concatenation
          cat cloudapps.crt cloudapps.key $CA/ca.crt > /etc/origin/master/cloudapps.router.pem
    (21e)
          oadm router trainingrouter --replicas=2 \
            --credentials='/etc/origin/master/openshift-router.kubeconfig' \
            --service-account=router --stats-password=\'r3dh@t1!'
    (21f) seperate terminal monitor BUILD process:
          oc get pod --all-namespaces -o wide  [CTRL-C to abort]
    (21g) VERIFY complete:
          oadm manage-node
